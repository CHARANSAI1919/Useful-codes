{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0798020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Representation:\n",
      "Features: ['and' 'antibiotics' 'as' 'breath' 'bronchitis' 'confirmed' 'cough'\n",
      " 'diagnosis' 'elevated' 'examination' 'inhaler' 'of' 'patient' 'physical'\n",
      " 'prescribed' 'presented' 'revealed' 'shortness' 'symptoms' 'temperature'\n",
      " 'wheezing' 'with']\n",
      "BoW Matrix:\n",
      " [[1 0 0 1 0 0 1 0 0 0 0 2 1 0 0 1 0 1 1 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the medical transcript and transform it into BoW representation\n",
    "bow_matrix = vectorizer.fit_transform(medical_transcript)\n",
    "\n",
    "# Convert the BoW matrix to an array for better readability\n",
    "bow_array = bow_matrix.toarray()\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the BoW representation\n",
    "print(\"Bag of Words (BoW) Representation:\")\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"BoW Matrix:\\n\", bow_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e276771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      "Features: ['and' 'antibiotics' 'as' 'breath' 'bronchitis' 'confirmed' 'cough'\n",
      " 'diagnosis' 'elevated' 'examination' 'inhaler' 'of' 'patient' 'physical'\n",
      " 'prescribed' 'presented' 'revealed' 'shortness' 'symptoms' 'temperature'\n",
      " 'wheezing' 'with']\n",
      "TF-IDF Matrix:\n",
      " [[0.17531933 0.         0.         0.29684142 0.         0.\n",
      "  0.29684142 0.         0.         0.         0.         0.59368285\n",
      "  0.29684142 0.         0.         0.29684142 0.         0.29684142\n",
      "  0.29684142 0.         0.         0.29684142]\n",
      " [0.2344005  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.39687454 0.39687454 0.         0.\n",
      "  0.         0.39687454 0.         0.         0.39687454 0.\n",
      "  0.         0.39687454 0.39687454 0.        ]\n",
      " [0.21786941 0.36888498 0.36888498 0.         0.36888498 0.36888498\n",
      "  0.         0.36888498 0.         0.         0.36888498 0.\n",
      "  0.         0.         0.36888498 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the medical transcript and transform it into TF-IDF representation\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(medical_transcript)\n",
    "\n",
    "# Convert the TF-IDF matrix to an array for better readability\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc30fad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Tokenize the medical transcript\n",
    "tokenized_transcript = [word_tokenize(sentence.lower()) for sentence in medical_transcript]\n",
    "\n",
    "# Train Word2Vec model on the tokenized medical transcript\n",
    "word2vec_model = Word2Vec(tokenized_transcript, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Print the vector representation of each word\n",
    "print(\"Word Embeddings:\")\n",
    "for word in word2vec_model.wv.index_to_key:\n",
    "    print(word, \":\", word2vec_model.wv[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d97df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Word 'cough.' not found in GloVe vocabulary.\n",
      "Word 'pneumonia.' not found in GloVe vocabulary.\n",
      "Mean Embedding: [-1.02584451e-01  7.92255998e-03  3.10465693e-01  1.03223354e-01\n",
      " -1.36028349e-01  2.85505563e-01  2.39101991e-01  1.67928442e-01\n",
      " -2.67617822e-01 -1.03004426e-01 -2.67057031e-01 -5.42732747e-03\n",
      "  4.66193825e-01  2.11903751e-01  6.56169236e-01 -9.69854370e-02\n",
      "  6.66448921e-02 -2.61384994e-01 -1.22886449e-01 -1.60737187e-01\n",
      " -2.75147647e-01 -3.15886050e-01 -2.05300465e-01  1.46979541e-01\n",
      "  4.58371118e-02  1.38336673e-01  4.18594331e-01 -3.73859525e-01\n",
      " -1.19682997e-01 -2.34338894e-01  1.60960123e-01  3.93518895e-01\n",
      " -1.06725112e-01  1.61731556e-01 -1.46150663e-01  1.59684867e-01\n",
      "  1.51260555e-01  1.25550210e-01 -2.95739532e-01  6.16921037e-02\n",
      " -3.28458577e-01  1.24426335e-01  3.54154408e-02 -4.29866672e-01\n",
      "  6.43779933e-02  1.58677876e-01  9.24467742e-02 -2.92243838e-01\n",
      " -3.62777710e-03 -4.24950004e-01  3.06643337e-01 -1.36291683e-01\n",
      "  4.96131144e-02  8.28071177e-01  1.04464954e-02 -1.84579551e+00\n",
      "  1.63795557e-02 -2.36416548e-01  7.91922808e-01  5.69337666e-01\n",
      "  2.60771322e-03  9.92653310e-01  1.89885590e-02  6.06366657e-02\n",
      "  4.00213867e-01  2.31014326e-01  2.75297761e-01 -7.25389179e-03\n",
      "  3.52691680e-01 -1.09083883e-01 -3.25252205e-01  1.71839986e-02\n",
      " -2.39482343e-01 -1.92381069e-02  4.61556554e-01  3.54487807e-01\n",
      " -2.42743433e-01 -4.27411087e-02 -8.78251076e-01 -3.28389674e-01\n",
      "  5.29897451e-01  2.43993342e-01 -3.71153414e-01  2.30856866e-01\n",
      " -1.58581114e+00 -1.86888739e-01  1.37502015e-01  2.45281104e-02\n",
      " -5.05152047e-01 -1.48035754e-02 -9.06685964e-05 -1.92762658e-01\n",
      "  4.34707366e-02  3.19339603e-01 -1.09250344e-01  3.77076656e-01\n",
      " -2.23261118e-01 -6.40427828e-01  4.79888856e-01 -2.27942646e-01]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GloVe word vectors\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Example medical transcript\n",
    "medical_transcript = \"Patient presented with symptoms of fever and cough. Diagnosis revealed pneumonia.\"\n",
    "\n",
    "# Tokenize the medical transcript\n",
    "tokens = medical_transcript.lower().split()\n",
    "\n",
    "# Initialize an empty list to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through tokens and get GloVe embeddings\n",
    "for token in tokens:\n",
    "    try:\n",
    "        embedding = glove_model[token]\n",
    "        embeddings.append(embedding)\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        print(f\"Word '{token}' not found in GloVe vocabulary.\")\n",
    "\n",
    "# Convert the list of embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Calculate the mean embedding\n",
    "mean_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "print(\"Mean Embedding:\", mean_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41ac939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
      "Word 'cough.' not found in FastText vocabulary.\n",
      "Word 'pneumonia.' not found in FastText vocabulary.\n",
      "Mean Embedding: [-1.15479492e-02  1.09176766e-02  6.12921081e-03  2.10567117e-02\n",
      " -1.70978010e-02 -1.76176019e-02 -7.24244025e-03 -1.05920002e-01\n",
      "  1.07433334e-04  5.74176665e-03 -2.19144449e-02 -4.38010879e-02\n",
      "  4.31474783e-02 -3.20506990e-02 -1.56641100e-02 -1.58629343e-02\n",
      "  7.13680312e-02  1.67137105e-02  8.18797722e-02  1.85287651e-02\n",
      " -2.26281881e-02 -5.41232247e-03 -5.98284416e-03  7.47917742e-02\n",
      " -1.34712793e-02 -1.67029221e-02 -2.20218785e-02  3.46966349e-02\n",
      "  7.08832443e-02  2.07872465e-02 -8.33849981e-03  2.12269053e-02\n",
      "  1.70863513e-02 -4.29130457e-02  1.48584531e-03  2.18419880e-02\n",
      " -3.90627645e-02  1.13719329e-02  2.87994351e-02  1.65983085e-02\n",
      " -6.79734442e-03 -3.78390364e-02 -4.23700130e-03 -4.55022231e-03\n",
      "  1.62924430e-03  1.81227550e-02  2.74424013e-02  1.77063867e-02\n",
      " -1.49371773e-02  3.79516929e-03  1.16935940e-02 -1.09610008e-02\n",
      " -2.04605982e-02 -2.99371779e-02 -5.16169891e-02  2.84859445e-02\n",
      "  1.11929774e-02  2.55134422e-03 -6.96052611e-02  2.43777875e-03\n",
      "  7.52624869e-03 -1.54004330e-02  1.40765667e-01 -1.74765568e-02\n",
      "  1.73174348e-02 -4.03532200e-03  1.30053433e-02  8.35783314e-03\n",
      " -2.21177004e-02 -3.58515605e-03 -3.84847783e-02  8.52664374e-03\n",
      "  6.99497759e-02  1.01770689e-04 -1.12298103e-02  1.12341447e-02\n",
      " -1.27216661e-02 -3.08182202e-02 -1.46800326e-02 -1.94617622e-02\n",
      " -1.46902334e-02  5.89246396e-03 -2.35285573e-02  5.54894470e-02\n",
      " -1.90417760e-03 -1.92875769e-02  8.38386640e-03 -1.88908111e-02\n",
      " -4.16116901e-02 -2.83936653e-02  1.33952992e-02  8.50166567e-03\n",
      " -1.16756998e-01 -2.08283775e-02 -8.32388923e-03  2.46189889e-02\n",
      "  8.55655596e-03 -1.58078801e-02  2.06668861e-02  2.24396959e-02\n",
      "  2.30617449e-02  1.91129912e-02 -5.79231000e-03  2.15440579e-02\n",
      "  2.83220485e-02 -1.02439791e-01  6.00951072e-03 -3.20974030e-02\n",
      " -3.03718876e-02  9.06904414e-03 -4.95547755e-03  7.68927857e-02\n",
      "  7.40998834e-02  2.46846657e-02  2.16129050e-02  2.07211301e-02\n",
      "  5.35762263e-03 -5.58020081e-03 -2.97348369e-02 -1.82601884e-02\n",
      " -3.31962737e-03  1.53081119e-03  3.29670645e-02 -2.71503888e-02\n",
      "  3.99913341e-02 -8.64315499e-03 -3.73553485e-05 -8.24815556e-02\n",
      "  1.97306219e-02  1.18093558e-01  2.93317437e-02 -1.85315683e-02\n",
      " -5.19831991e-03 -6.20786659e-03  8.72138044e-05  4.89223329e-03\n",
      "  1.29158897e-02  7.39986002e-02  3.51793319e-02  3.56348902e-02\n",
      "  5.89292198e-02 -1.22557674e-02 -3.30554545e-02 -3.03712278e-03\n",
      " -1.06496327e-02 -1.84236001e-02  5.22604398e-03 -2.11020350e-03\n",
      "  1.02982558e-02  3.30802263e-03  1.83433127e-02  2.27216668e-02\n",
      "  2.48888228e-02 -7.40948841e-02 -1.40382303e-03  2.90205493e-03\n",
      " -1.13797770e-03  4.32328461e-03  9.55959968e-03 -2.10880414e-02\n",
      "  3.09734121e-02 -2.09048670e-02 -5.97237349e-02  8.68995558e-04\n",
      "  2.07999721e-04 -2.44134106e-02  1.79450996e-02 -3.18693966e-02\n",
      " -1.12255448e-02 -1.41497534e-02 -3.64499958e-03 -3.12006637e-03\n",
      " -1.13157444e-02 -4.54576407e-03  1.60422679e-02  4.26755548e-02\n",
      " -3.68816662e-03 -2.09129993e-02 -3.55956405e-02 -1.90903116e-02\n",
      "  9.30524524e-03  8.36643297e-03 -2.04920024e-03 -2.60686711e-03\n",
      "  1.57424249e-02 -3.43960188e-02  2.74064764e-02  1.38529882e-01\n",
      " -1.86005905e-02 -2.29145307e-03  2.55196467e-02  6.16395473e-02\n",
      " -1.06767565e-01 -2.37749536e-02 -1.40558444e-02  3.01768351e-02\n",
      "  2.44202139e-03  3.17423195e-02  5.53994486e-03  1.35902436e-02\n",
      " -1.53692111e-01  6.71949983e-02 -3.05291079e-02 -2.42862776e-02\n",
      " -6.85013423e-04 -2.62756716e-03  8.09251145e-03 -2.66740564e-02\n",
      " -3.34949344e-02 -3.94687243e-02  1.15842000e-01  2.72750370e-02\n",
      "  2.40012035e-02  2.22965553e-02  3.31313349e-02 -7.30238948e-03\n",
      "  1.28226914e-02  5.40756760e-03 -5.13321068e-03 -4.52275425e-02\n",
      " -2.15134770e-02 -4.71427198e-03 -2.64312029e-02  1.04089111e-01\n",
      "  3.00830211e-02  3.44433226e-02  1.01131219e-02  8.07815492e-02\n",
      " -1.16548724e-02 -7.50686750e-02  1.35452440e-02 -7.42007345e-02\n",
      " -8.21707770e-02 -8.23533256e-03  1.68828238e-02  3.19561549e-02\n",
      "  5.25500067e-03 -5.33584505e-03  3.43071204e-03 -2.52883658e-02\n",
      "  6.69007227e-02 -7.14924419e-03 -1.55268656e-02 -4.62411175e-04\n",
      " -1.00775003e-01  1.94641706e-02 -5.17505594e-03  1.17165446e-02\n",
      " -1.76090226e-02 -3.64379883e-02  5.88631108e-02  2.46525779e-02\n",
      "  3.03297639e-02  7.46728061e-03 -3.00355535e-03 -4.22268827e-03\n",
      " -1.44831557e-02  4.99371924e-02  1.95152108e-02 -1.89631782e-03\n",
      " -3.28204669e-02  7.15429895e-03 -2.39794888e-02  2.56131142e-02\n",
      "  4.84481826e-03 -3.68639976e-02  1.38271116e-02  5.07215001e-02\n",
      " -7.66364811e-03 -1.44887008e-02 -2.30575446e-02  9.73244384e-02\n",
      " -1.42714337e-01 -3.68934870e-02  3.61893140e-02 -7.20704421e-02\n",
      "  4.11564438e-03 -7.85459951e-03 -4.92617628e-03  1.79169036e-03\n",
      " -3.34942192e-02  2.40848232e-02  8.72651115e-03 -1.80422224e-03\n",
      "  3.01197767e-02  1.21591110e-02 -1.74301006e-02  6.34453353e-03\n",
      " -8.49016663e-03  1.13306111e-02  1.23160668e-02  1.33883888e-02\n",
      " -2.51967106e-02 -2.16837451e-02 -1.29220104e-02  8.09646305e-03\n",
      " -9.36366618e-04  1.84804259e-04  9.90474783e-03  8.11906531e-03]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained FastText word vectors\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# Example medical transcript\n",
    "medical_transcript = \"Patient presented with symptoms of fever and cough. Diagnosis revealed pneumonia.\"\n",
    "\n",
    "# Tokenize the medical transcript\n",
    "tokens = medical_transcript.lower().split()\n",
    "\n",
    "# Initialize an empty list to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through tokens and get FastText embeddings\n",
    "for token in tokens:\n",
    "    try:\n",
    "        embedding = fasttext_model[token]\n",
    "        embeddings.append(embedding)\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        print(f\"Word '{token}' not found in FastText vocabulary.\")\n",
    "\n",
    "# Convert the list of embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Calculate the mean embedding\n",
    "mean_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "print(\"Mean Embedding:\", mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17449a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character n-gram Representation:\n",
      "Features: [' a' ' an' ' and' ' and ' ' ant' ' anti' ' as' ' as ' ' as b' ' b' ' br'\n",
      " ' bre' ' brea' ' bro' ' bron' ' c' ' co' ' con' ' conf' ' cou' ' coug'\n",
      " ' e' ' el' ' ele' ' elev' ' ex' ' exa' ' exam' ' i' ' in' ' inh' ' inha'\n",
      " ' o' ' of' ' of ' ' of b' ' of c' ' p' ' pr' ' pre' ' pres' ' r' ' re'\n",
      " ' rev' ' reve' ' s' ' sh' ' sho' ' shor' ' sy' ' sym' ' symp' ' t' ' te'\n",
      " ' tem' ' temp' ' w' ' wh' ' whe' ' whee' ' wi' ' wit' ' with' ', ' ', p'\n",
      " ', pr' ', pre' 'ag' 'agn' 'agno' 'agnos' 'al' 'al ' 'al e' 'al ex' 'ale'\n",
      " 'aled' 'aled ' 'aler' 'aler.' 'am' 'ami' 'amin' 'amina' 'an' 'and' 'and '\n",
      " 'and i' 'and s' 'and w' 'ant' 'anti' 'antib' 'as' 'as ' 'as b' 'as br'\n",
      " 'at' 'ate' 'ated' 'ated ' 'ath' 'ath.' 'ati' 'atie' 'atien' 'atio'\n",
      " 'ation' 'atu' 'atur' 'ature' 'be' 'bed' 'bed ' 'bed a' 'bi' 'bio' 'biot'\n",
      " 'bioti' 'br' 'bre' 'brea' 'breat' 'bro' 'bron' 'bronc' 'ca' 'cal' 'cal '\n",
      " 'cal e' 'ch' 'chi' 'chit' 'chiti' 'co' 'con' 'conf' 'confi' 'cou' 'coug'\n",
      " 'cough' 'cr' 'cri' 'crib' 'cribe' 'cs' 'cs ' 'cs a' 'cs an' 'd ' 'd a'\n",
      " 'd an' 'd ant' 'd as' 'd as ' 'd e' 'd el' 'd ele' 'd i' 'd in' 'd inh'\n",
      " 'd s' 'd sh' 'd sho' 'd t' 'd te' 'd tem' 'd w' 'd wh' 'd whe' 'd wi'\n",
      " 'd wit' 'di' 'dia' 'diag' 'diagn' 'e ' 'e a' 'e an' 'e and' 'ea' 'eal'\n",
      " 'eale' 'ealed' 'eat' 'eath' 'eath.' 'ed' 'ed ' 'ed a' 'ed an' 'ed as'\n",
      " 'ed e' 'ed el' 'ed t' 'ed te' 'ed w' 'ed wi' 'ee' 'eez' 'eezi' 'eezin'\n",
      " 'el' 'ele' 'elev' 'eleva' 'em' 'emp' 'empe' 'emper' 'en' 'ent' 'ent '\n",
      " 'ent p' 'ente' 'ented' 'er' 'er.' 'era' 'erat' 'eratu' 'es' 'esc' 'escr'\n",
      " 'escri' 'ese' 'esen' 'esent' 'ess' 'ess ' 'ess o' 'ev' 'eva' 'evat'\n",
      " 'evate' 'eve' 'evea' 'eveal' 'ex' 'exa' 'exam' 'exami' 'ez' 'ezi' 'ezin'\n",
      " 'ezing' 'f ' 'f b' 'f br' 'f bre' 'f c' 'f co' 'f cou' 'fi' 'fir' 'firm'\n",
      " 'firme' 'g.' 'gh' 'gh ' 'gh a' 'gh an' 'gn' 'gno' 'gnos' 'gnosi' 'h '\n",
      " 'h a' 'h an' 'h and' 'h s' 'h sy' 'h sym' 'h.' 'ha' 'hal' 'hale' 'haler'\n",
      " 'he' 'hee' 'heez' 'heezi' 'hi' 'hit' 'hiti' 'hitis' 'ho' 'hor' 'hort'\n",
      " 'hortn' 'hy' 'hys' 'hysi' 'hysic' 'ia' 'iag' 'iagn' 'iagno' 'ib' 'ibe'\n",
      " 'ibed' 'ibed ' 'ibi' 'ibio' 'ibiot' 'ic' 'ica' 'ical' 'ical ' 'ics'\n",
      " 'ics ' 'ics a' 'ie' 'ien' 'ient' 'ient ' 'in' 'ina' 'inat' 'inati' 'ing'\n",
      " 'ing.' 'inh' 'inha' 'inhal' 'io' 'ion' 'ion ' 'ion r' 'iot' 'ioti'\n",
      " 'iotic' 'ir' 'irm' 'irme' 'irmed' 'is' 'is ' 'is c' 'is co' 'is,' 'is, '\n",
      " 'is, p' 'it' 'ith' 'ith ' 'ith s' 'iti' 'itis' 'itis,' 'l ' 'l e' 'l ex'\n",
      " 'l exa' 'le' 'led' 'led ' 'led e' 'ler' 'ler.' 'lev' 'leva' 'levat' 'me'\n",
      " 'med' 'med ' 'med a' 'mi' 'min' 'mina' 'minat' 'mp' 'mpe' 'mper' 'mpera'\n",
      " 'mpt' 'mpto' 'mptom' 'ms' 'ms ' 'ms o' 'ms of' 'n ' 'n r' 'n re' 'n rev'\n",
      " 'na' 'nat' 'nati' 'natio' 'nc' 'nch' 'nchi' 'nchit' 'nd' 'nd ' 'nd i'\n",
      " 'nd in' 'nd s' 'nd sh' 'nd w' 'nd wh' 'ne' 'nes' 'ness' 'ness ' 'nf'\n",
      " 'nfi' 'nfir' 'nfirm' 'ng' 'ng.' 'nh' 'nha' 'nhal' 'nhale' 'no' 'nos'\n",
      " 'nosi' 'nosis' 'nt' 'nt ' 'nt p' 'nt pr' 'nte' 'nted' 'nted ' 'nti'\n",
      " 'ntib' 'ntibi' 'of' 'of ' 'of b' 'of br' 'of c' 'of co' 'om' 'oms' 'oms '\n",
      " 'oms o' 'on' 'on ' 'on r' 'on re' 'onc' 'onch' 'onchi' 'onf' 'onfi'\n",
      " 'onfir' 'or' 'ort' 'ortn' 'ortne' 'os' 'osi' 'osis' 'osis ' 'ot' 'oti'\n",
      " 'otic' 'otics' 'ou' 'oug' 'ough' 'ough ' 'pa' 'pat' 'pati' 'patie' 'pe'\n",
      " 'per' 'pera' 'perat' 'ph' 'phy' 'phys' 'physi' 'pr' 'pre' 'pres' 'presc'\n",
      " 'prese' 'pt' 'pto' 'ptom' 'ptoms' 'r.' 'ra' 'rat' 'ratu' 'ratur' 're'\n",
      " 're ' 're a' 're an' 'rea' 'reat' 'reath' 'res' 'resc' 'rescr' 'rese'\n",
      " 'resen' 'rev' 'reve' 'revea' 'ri' 'rib' 'ribe' 'ribed' 'rm' 'rme' 'rmed'\n",
      " 'rmed ' 'ro' 'ron' 'ronc' 'ronch' 'rt' 'rtn' 'rtne' 'rtnes' 's ' 's a'\n",
      " 's an' 's and' 's b' 's br' 's bro' 's c' 's co' 's con' 's o' 's of'\n",
      " 's of ' 's,' 's, ' 's, p' 's, pr' 'sc' 'scr' 'scri' 'scrib' 'se' 'sen'\n",
      " 'sent' 'sente' 'sh' 'sho' 'shor' 'short' 'si' 'sic' 'sica' 'sical' 'sis'\n",
      " 'sis ' 'sis c' 'ss' 'ss ' 'ss o' 'ss of' 'sy' 'sym' 'symp' 'sympt' 't '\n",
      " 't p' 't pr' 't pre' 'te' 'ted' 'ted ' 'ted t' 'ted w' 'tem' 'temp'\n",
      " 'tempe' 'th' 'th ' 'th s' 'th sy' 'th.' 'ti' 'tib' 'tibi' 'tibio' 'tic'\n",
      " 'tics' 'tics ' 'tie' 'tien' 'tient' 'tio' 'tion' 'tion ' 'tis' 'tis,'\n",
      " 'tis, ' 'tn' 'tne' 'tnes' 'tness' 'to' 'tom' 'toms' 'toms ' 'tu' 'tur'\n",
      " 'ture' 'ture ' 'ug' 'ugh' 'ugh ' 'ugh a' 'ur' 'ure' 'ure ' 'ure a' 'va'\n",
      " 'vat' 'vate' 'vated' 've' 'vea' 'veal' 'veale' 'wh' 'whe' 'whee' 'wheez'\n",
      " 'wi' 'wit' 'with' 'with ' 'xa' 'xam' 'xami' 'xamin' 'ym' 'ymp' 'ympt'\n",
      " 'ympto' 'ys' 'ysi' 'ysic' 'ysica' 'zi' 'zin' 'zing' 'zing.']\n",
      "Character n-gram Matrix:\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [3 2 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Define the range of n-grams (e.g., 2-grams to 5-grams)\n",
    "min_n = 2\n",
    "max_n = 5\n",
    "\n",
    "# Initialize the CountVectorizer with character n-grams\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(min_n, max_n))\n",
    "\n",
    "# Fit the vectorizer to the medical transcript and transform it into character n-gram representation\n",
    "char_ngram_matrix = vectorizer.fit_transform(medical_transcript)\n",
    "\n",
    "# Convert the character n-gram matrix to an array for better readability\n",
    "char_ngram_array = char_ngram_matrix.toarray()\n",
    "\n",
    "# Get the feature names (character n-grams)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the character n-gram representation\n",
    "print(\"Character n-gram Representation:\")\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"Character n-gram Matrix:\\n\", char_ngram_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c147248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte Pair Encoding Result:\n",
      "Patient_presented_with_symptoms_of_cough_and_shortness_of_breath._Physical_examination_revealed_elevated_temperature_and_wheezing._Diagnosis_confirmed_as_bronchitis,_prescribed_antibiotics_and_inhaler.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Concatenate all sentences into a single string\n",
    "text = ' '.join(medical_transcript)\n",
    "\n",
    "# Define the number of iterations for BPE algorithm\n",
    "num_iterations = 10\n",
    "\n",
    "# Perform Byte Pair Encoding\n",
    "for _ in range(num_iterations):\n",
    "    # Calculate character frequencies\n",
    "    char_freq = Counter(text)\n",
    "    \n",
    "    # Find the most common character pair\n",
    "    most_common_pair = max(char_freq, key=char_freq.get)\n",
    "    \n",
    "    # Replace the most common character pair with a new symbol\n",
    "    new_symbol = most_common_pair.replace(' ', '_')\n",
    "    text = re.sub(re.escape(most_common_pair), new_symbol, text)\n",
    "\n",
    "# Print the result of Byte Pair Encoding\n",
    "print(\"Byte Pair Encoding Result:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a146ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashed Vector Representation:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Initialize the HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=1000, alternate_sign=False)\n",
    "\n",
    "# Transform the medical transcript into a hashed vector representation\n",
    "hashed_vector = vectorizer.transform(medical_transcript)\n",
    "\n",
    "# Convert the hashed vector to an array for better readability\n",
    "hashed_array = hashed_vector.toarray()\n",
    "\n",
    "# Print the hashed vector representation\n",
    "print(\"Hashed Vector Representation:\")\n",
    "print(hashed_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4cbd078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siva7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Vector for the First Sentence:\n",
      "[-1.1136020e-02 -4.8494600e-03  1.6627514e-03  7.4614370e-03\n",
      "  6.6040161e-06 -7.5934301e-03 -5.9609051e-04  2.8972214e-03\n",
      " -8.3302148e-03  1.2111573e-03  2.9504893e-03 -1.8201616e-03\n",
      " -6.3358005e-03 -7.2267842e-03  3.1805965e-03 -3.8422627e-04\n",
      "  4.0237317e-03  3.5190799e-03 -7.6869172e-03 -6.6963811e-03\n",
      "  5.5732892e-04 -2.4530129e-03 -8.0813887e-04  9.1940360e-03\n",
      " -5.4831645e-03  2.4649317e-03 -1.3437697e-02  1.5908031e-03\n",
      " -9.8421471e-04 -1.8603043e-03  1.5207157e-04  3.4204770e-03\n",
      " -2.6636729e-03 -4.3324213e-03 -8.9364303e-03  6.1663757e-03\n",
      " -1.4795577e-03 -2.2712150e-03 -9.2690848e-03 -4.3508477e-04\n",
      "  1.0315078e-02  2.3778703e-03  3.6113393e-03 -4.3615983e-03\n",
      "  2.5333001e-03 -2.8603885e-03  2.1185563e-03  3.3065800e-03\n",
      "  9.8981638e-04 -8.1872474e-03 -4.1686646e-03 -4.0235552e-03\n",
      " -1.0311445e-02 -9.6029993e-03 -5.7981513e-03  1.1725518e-02\n",
      "  4.0487880e-03  5.4513095e-03 -4.0713158e-03  2.8072281e-03\n",
      "  3.2607757e-03  1.1349788e-02  9.6196206e-03 -2.7772204e-03\n",
      "  3.5095258e-04  1.2256765e-03  4.0428890e-03  3.0731827e-03\n",
      " -5.1173023e-03 -3.9294232e-03 -1.2740761e-02  2.0194899e-03\n",
      " -8.6008711e-03 -6.1422755e-04  1.4299925e-03 -1.3614255e-03\n",
      " -8.3530776e-04 -4.7336458e-03 -7.5765443e-03 -5.1825121e-04\n",
      " -3.8349689e-03  2.3771576e-03 -1.1882437e-02  1.1466148e-03\n",
      "  8.8335236e-04  5.4999045e-03  6.0829651e-03 -9.3653128e-03\n",
      "  2.0345650e-04  3.5168449e-03 -1.4567061e-02  9.8295324e-03\n",
      " -1.3903463e-02  4.2257621e-04  8.7366457e-04  7.7339979e-03\n",
      " -2.6731385e-04 -8.0684517e-03  7.4290121e-03  8.0736009e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Tokenize the medical transcript\n",
    "tokenized_transcript = [word_tokenize(sentence.lower()) for sentence in medical_transcript]\n",
    "\n",
    "# Tag each tokenized sentence with a unique ID\n",
    "tagged_data = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(tokenized_transcript)]\n",
    "\n",
    "# Initialize and train the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Infer vectors for the medical transcript\n",
    "inferred_vector = model.infer_vector(tokenized_transcript[0])\n",
    "\n",
    "# Print the inferred vector\n",
    "print(\"Inferred Vector for the First Sentence:\")\n",
    "print(inferred_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e45962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siva7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic 1: 0.075*\".\" + 0.075*\"and\" + 0.074*\"temperature\" + 0.074*\"revealed\" + 0.074*\"wheezing\" + 0.074*\"elevated\" + 0.074*\"examination\" + 0.074*\"physical\" + 0.026*\"of\" + 0.025*\"shortness\"\n",
      "Topic 2: 0.076*\"and\" + 0.076*\".\" + 0.076*\"of\" + 0.045*\"as\" + 0.045*\"antibiotics\" + 0.045*\"bronchitis\" + 0.045*\",\" + 0.045*\"prescribed\" + 0.045*\"diagnosis\" + 0.045*\"inhaler\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample medical transcript\n",
    "medical_transcript = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Tokenize the medical transcript\n",
    "tokenized_transcript = [word_tokenize(sentence.lower()) for sentence in medical_transcript]\n",
    "\n",
    "# Create a dictionary from the tokenized transcript\n",
    "dictionary = Dictionary(tokenized_transcript)\n",
    "\n",
    "# Create a bag of words representation of the transcript\n",
    "bow_corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_transcript]\n",
    "\n",
    "# Initialize and train the LDA model\n",
    "lda_model = LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=2, passes=10)\n",
    "\n",
    "# Print the topics and their top words\n",
    "print(\"LDA Topics:\")\n",
    "for topic_id, topic_words in lda_model.print_topics():\n",
    "    print(f\"Topic {topic_id + 1}: {topic_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38611d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
